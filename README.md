<!-- TOC START -->
## 목차 (Table of Contents)

### [01_퍼셉트론.ipynb](01_%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0.ipynb)
  - 01 퍼셉트론이란?
  - 02 단순한 논리 회로
    - 02.01 AND 게이트
    - 02.01 NAND 게이트와 OR 게이트
  - 03 퍼셉트론 구현하기
    - 03.01 간단한 구현부터
    - 03.02 가중치와 편향 도입
    - 03.03 가중치와 편향 구현하기
  - 04 퍼셉트론의 한계
    - 04.01 XOR 게이트
    - 04.02 선형과 비선형
  - 05 다층 퍼셉트론이 출동한다면
    - 05.01 기존 게이트 조합하기
    - 05.02 XOR 게이트 구현하기
  - 06 NAND에서 컴퓨터까지

### [02_신경망.ipynb](02_%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb)
  - 01 퍼셉트론에서 신경망으로
    - 01.01 신경망의 예
    - 01.02 퍼셉트론 복습
    - 01.03 활성화 함수의 등장
  - 02 활성화 함수
    - 02.01 시그모이드 함수
    - 02.02 계단 함수 구현하기
    - 02.03 계단 함수의 그래프
    - 02.04 시그모이드 함수 구현하기
    - 02.05 시그모이드 함수와 계단 함수 비교
    - 02.06 비선형 함수
    - 02.07 ReLU 함수
  - 03 다차원 배열의 계산
    - 03.01 다차원 배열
    - 03.02 행렬의 곱
    - 03.03 신경망에서의 행렬 곱
  - 04 3층 신경망 구현하기
    - 04.01 표기법 설명
    - 04.02 각 층의 신호 전달 구현하기
    - 04.03 구현 정리
  - 05 출력층 설계하기
    - 05.01 항등 함수와 소프트맥스 함수 구현하기
    - 05.02 소프트맥스 함수 구현 시 주의점
    - 05.03 소프트맥스 함수의 특징
    - 05.04 출력층의 뉴런 수 정하기
  - 06 손글씨 숫자 인식
    - 06.01 MNIST 데이터셋
    - 06.02 신경망의 추론 처리
    - 06.03 배치 처리

### [03_신경망_학습.ipynb](03_%EC%8B%A0%EA%B2%BD%EB%A7%9D_%ED%95%99%EC%8A%B5.ipynb)
  - 01 데이터에서 학습한다!
    - 01.01 데이터 주도 학습
    - 01.02 훈련 데이터와 시험 데이터
  - 02 손실 함수 (loss function)
    - 02.01 오차제곱합(SSE; sum of squres of error)
    - 02.02 교차 엔트로피 오차 (CEE; cross entropy error)
    - 02.03 미니배치 학습
    - 02.04 (배치용) 교차 엔트로피 오차 구현하기
    - 02.05 왜 손실 함수를 설정하는가?

<!-- TOC END -->



